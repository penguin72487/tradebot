import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt

import xgboost as xgb
import lightgbm as lgb
import catboost as cb
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge, LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.linear_model import BayesianRidge
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.base import clone  # åŠ åœ¨æœ€ä¸Šé¢

import warnings
from sklearn.exceptions import ConvergenceWarning

warnings.filterwarnings("ignore", category=ConvergenceWarning)
# ä½¿ç”¨ç›¸å°è·¯å¾‘è®€å– CSV
file_path = os.path.join(os.path.dirname(__file__), 'final_features.csv')
base_dir = os.path.dirname(file_path)
result_dir = os.path.join(base_dir, 'results_Ex')
os.makedirs(result_dir, exist_ok=True)  # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
df = pd.read_csv(file_path, dtype={'year_month': str})


df['year_month'] = df['year_month'].astype(str)

# çœ‹çœ‹ year_month æ¬„ä½ä¸­ä¸æ˜¯ 6 ç¢¼æ•¸å­—çš„æœ‰å“ªäº›
invalid = df[~df['year_month'].astype(str).str.match(r'^\d{6}$')]

if not invalid.empty:
    print("âš ï¸ ç™¼ç¾ä»¥ä¸‹ year_month æ¬„ä½æ ¼å¼ç•°å¸¸çš„è³‡æ–™ï¼š")
    print(invalid[['stock_id', 'year_month']].head(10))  # å…ˆå°å‰10ç­†
else:
    print("âœ… æ‰€æœ‰ year_month æ ¼å¼æ­£å¸¸å–µï½")
df['year'] = df['year_month'].str[:4].astype(int)
df['return'] = df['return'] / 100

df['current_return'] = df['return'].shift(1)  # ç•¶å‰å ±é…¬ç‡
df['current_return_label'] = (df['current_return'] > 0).astype(int)  # ç•¶å‰å ±é…¬ç‡æ¨™ç±¤

# ç‰¹å¾µæ¬„ä½

# åŸå§‹æ•¸å€¼æ¬„ä½ï¼ˆæ’é™¤éæ•¸å€¼æˆ–æ¨™ç±¤ï¼‰
base_features = df.drop(columns=['year_month', 'year', 'return', 'return_label']) \
                  .select_dtypes(include=[np.number]).columns.tolist()

# åŠ å…¥è®ŠåŒ–ç‡ï¼ˆå·®åˆ†æ¯”ä¾‹è®ŠåŒ–ï¼‰1~4éš
# åŠ å…¥è®ŠåŒ–ç‡ï¼ˆè‡ªè¨‚å‰ä¸€ç­†ç‚º 0 çš„è¡Œç‚ºï¼‰
chg_features = {}

for col in base_features:
    for k in range(1, 7):
        prev = df[col].shift(k)
        safe_prev = prev.replace(0, np.nan)
        change = (df[col] - prev) / safe_prev
        chg_features[f"{col}_chg{k}"] = change.fillna(0)

# ä¸€æ¬¡æ€§åŠ å…¥æ‰€æœ‰æ–°æ¬„ä½ï¼Œé¿å… fragmentation
chg_df = pd.DataFrame(chg_features)
df = pd.concat([df.reset_index(drop=True), chg_df.reset_index(drop=True)], axis=1)

# å¦‚æœä½ å¾Œé¢è¦ç¹¼çºŒè™•ç† dfï¼Œä¹Ÿå¯ä»¥å»é™¤ fragmentation
df = df.copy()  # å»é™¤ internal block fragmentation



changed_features = df.drop(columns=['year_month', 'year', 'return', 'return_label']) \
                  .select_dtypes(include=[np.number]).columns.tolist()


from sklearn.preprocessing import (
    StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler,
    Normalizer, PowerTransformer, QuantileTransformer
)
# å»ºç«‹æ¨™æº–åŒ–ç‰ˆæœ¬
scalers = {
    'z': StandardScaler(),
    'minmax': MinMaxScaler(),
    'maxabs': MaxAbsScaler(),
    'robust': RobustScaler(),
    # 'l2norm': Normalizer(norm='l2'),
    'power': PowerTransformer(method='yeo-johnson'),
    'quantile': QuantileTransformer(output_distribution='normal', n_quantiles=100)
}
df = df.sort_values(by='year').reset_index(drop=True)

# åªä¿ç•™æœ‰ year æ¬„ä½çš„ row
years = sorted(df['year'].unique())

# å»ºç«‹æ¯ä¸€ç¨® scaler çš„æ»¾å‹•æ¨™æº–åŒ–çµæœ
for name, scaler_obj in scalers.items():
    print(f"ğŸ”§ æ­£åœ¨è™•ç†æ¨™æº–åŒ–æ–¹å¼ï¼š{name} ...")
    transformed_list = []
    
    for current_year in years:
        # å–å¾—ç•¶å‰å¹´ä»¥å‰çš„è³‡æ–™ç•¶ä½œ fit çš„ base
        fit_data = df[df['year'] <= current_year][changed_features]

        try:
            scaler = clone(scaler_obj)
            scaler.fit(fit_data)

            # transform ç•¶å¹´ç•¶ä¸‹çš„è³‡æ–™
            current_data = df[df['year'] == current_year][changed_features]
            transformed = scaler.transform(current_data)

            transformed_df = pd.DataFrame(
                transformed,
                columns=[f"{col}_{name}" for col in changed_features],
                index=current_data.index
            )
            transformed_list.append(transformed_df)

        except Exception as e:
            print(f"âš ï¸ {name} åœ¨ year={current_year} æ¨™æº–åŒ–å¤±æ•—ï¼š{e}")
            continue

    # åˆä½µæ‰€æœ‰å¹´åº¦çš„æ¨™æº–åŒ–çµæœ
    all_transformed = pd.concat(transformed_list).sort_index()
    df = pd.concat([df, all_transformed], axis=1)

print("âœ… æ»¾å‹•å¼æ¨™æº–åŒ–å®Œæˆå›‰ï½é¿å…æœªä¾†è³‡è¨Šå–µâ™¡")

# æ›´æ–° all_featuresï¼ˆé¸æ‰€æœ‰æ•¸å€¼ç‰¹å¾µï¼Œä¸å«æ¨™ç±¤ï¼‰
exclude = ['year_month', 'year', 'return', 'return_label']
all_features = df.drop(columns=exclude).select_dtypes(include=[np.number]).columns.tolist()

# ç¯©é¸å®Œæ•´å¹´ä»½
df_2024 = df[df['year'] == 2024] 
years = sorted(df['year'].unique())[1:-1]
df = df[df['year'].isin(years)]
# æ¯å¹´è‚¡ç¥¨æ•¸é‡çµ±è¨ˆ
yearly_stock_counts = df.groupby('year')['stock_id'].nunique()
print("æ¯å¹´è‚¡ç¥¨æ•¸é‡ï¼š")
print(yearly_stock_counts)

# æ¨¡å‹é›†åˆ
models = {
    # 'Ridge': Ridge(
    #     alpha=10.0,
    #     fit_intercept=True,
    #     solver='auto'
    # ),
    # 'KNN': KNeighborsRegressor(
    #     n_neighbors=7,
    #     weights='distance',
    #     algorithm='auto',
    #     leaf_size=20,
    #     p=2,
    #     metric='minkowski'
    # ),
    # 'ExtraTrees': ExtraTreesRegressor(
    #     n_estimators=300,
    #     max_depth=8,
    #     min_samples_split=5,
    #     min_samples_leaf=3,
    #     max_features='sqrt',
    #     bootstrap=True
    # ),
    # 'HistGB' : HistGradientBoostingRegressor(
    #     max_iter=300,
    #     learning_rate=0.05,
    #     max_depth=6,
    #     l2_regularization=0.1,
    #     early_stopping=True
    # ),
    'BayesianRidge': BayesianRidge(
        max_iter=300,
        tol=1e-4
    ),
    # 'Linear': LinearRegression(
    #     fit_intercept=True,
    #     copy_X=True
    # ),
    # 'RandomForest': RandomForestRegressor(
    #     n_estimators=300,
    #     max_depth=8,
    #     min_samples_split=5,
    #     min_samples_leaf=3,
    #     max_features='sqrt',
    #     bootstrap=True
    # ),
    # 'SVR': SVR(
    #     kernel='rbf',
    #     C=1.0,
    #     epsilon=0.01
    # ),
    # 'XGBoost': xgb.XGBRegressor(
    #     objective='reg:squarederror',
    #     max_depth=4, 
    #     eta=0.1, 
    #     n_estimators=300
    # ),
    # 'CatBoost': cb.CatBoostRegressor(
    #     iterations=500,
    #     learning_rate=0.03,
    #     depth=6,
    #     l2_leaf_reg=3,
    #     loss_function='RMSE',
    #     verbose=0
    # )
}

param_spaces = {
    'Ridge': {
        'alpha': (1e-3, 1e8),  # æ­£å‰‡åŒ–åƒæ•¸
        'fit_intercept': [True, False],
        'solver': ['auto', 'svd', 'cholesky', 'lsqr','sparse_cg', 'sag', 'saga'],
        'max_iter': (100, 1e8),  # æœ€å¤§è¿­ä»£æ¬¡æ•¸
    },
    'SVR': {
        'C': (1e-3, 1e6),              # æ­£å‰‡åŒ–åƒæ•¸
        'epsilon': (0.001, 1.0),        # å®¹è¨±èª¤å·®
        'kernel': ['rbf'],
        'gamma': ['scale', 'auto'],     # æ ¸å‡½æ•¸ä¿‚æ•¸
        'shrinking': [True, False]
    },
    'KNN': {
        'n_neighbors': (1, 200),
        'weights': ['uniform', 'distance'],
        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
        'leaf_size': (10, 1e8),
        'p': (1, 2),  # æ›¼å“ˆé “ or æ­å¼è·é›¢
        'metric': ['minkowski']
    },
    'ExtraTrees': {
        'n_estimators': (50, 500),
        'max_depth': (2, 30),
        'min_samples_split': (2, 20),
        'min_samples_leaf': (1, 10),
        'max_features': ['sqrt', 'log2'],
        'bootstrap': [True, False]
    },
    'HistGB': {
        'max_iter': (100, 1000),
        'learning_rate': (0.01, 0.2),
        'max_depth': (2, 12),
        'l2_regularization': (0.0, 5.0),
        'max_bins': (128, 255),
        'early_stopping': [True, False]
    },
    'BayesianRidge': {
        'max_iter': (100, 3000),
        'tol': (1e-6, 1),
        'alpha_1': (1e-7, 1e8),
        'alpha_2': (1e-7, 1e8),
        'lambda_1': (1e-7, 1e8),
        'lambda_2': (1e-7, 1e8),
        'fit_intercept': [True, False],
        'compute_score': [True, False],
        'copy_X': [True, False],
    },
    'Linear': {
        'fit_intercept': [True, False],
        'copy_X': [True, False],
        'positive': [True, False]
    },
    'RandomForest': {
        'n_estimators': (50, 500),
        'max_depth': (2, 30),
        'min_samples_split': (2, 20),
        'min_samples_leaf': (1, 10),
        'max_features': ['sqrt', 'log2'],
        'bootstrap': [True, False]
    },
    'XGBoost': {
        'objective': ['reg:squarederror'],
        'n_estimators': (50, 500),
        'max_depth': (2, 12),
        'learning_rate': (0.01, 0.3),  # eta
        'subsample': (0.5, 1.0),
        'colsample_bytree': (0.5, 1.0),
        'gamma': (0, 5.0),
        'reg_alpha': (0.0, 5.0),
        'reg_lambda': (0.0, 5.0),
        'booster': ['gbtree', 'dart']
    },
    'CatBoost': {
        'iterations': (100, 1000),
        'learning_rate': (0.01, 0.3),
        'depth': (3, 10),
        'l2_leaf_reg': (1, 10),
        'bootstrap_type': ['Bayesian', 'Bernoulli', 'MVS'],
        'bagging_temperature': (0.0, 1.0),
        'random_strength': (1e-9, 10.0),
        'loss_function': ['RMSE', 'MAE', 'Quantile', 'LogLinQuantile']
    }
}

def init_models_by_name (model_name, params):
    """
    æ ¹æ“šæ¨¡å‹åç¨±å’Œåƒæ•¸åˆå§‹åŒ–æ¨¡å‹ã€‚
    """
    if model_name == 'XGBoost':
        model = xgb.XGBRegressor(**params)
    elif model_name == 'CatBoost':
        model = cb.CatBoostRegressor(verbose=0, **params)
    elif model_name == 'SVR':
        model = SVR(**params)
    elif model_name == 'Ridge':
        model = Ridge(**params)
    elif model_name == 'BayesianRidge':
        model = BayesianRidge(**params)
    elif model_name == 'Linear':
        model = LinearRegression(**params)
    elif model_name == 'KNN':
        model = KNeighborsRegressor(**params)
    elif model_name == 'ExtraTrees':
        model = ExtraTreesRegressor(**params)
    elif model_name == 'RandomForest':
        model = RandomForestRegressor(**params)
    elif model_name == 'HistGB':
        model = HistGradientBoostingRegressor(**params)
    else:
        raise ValueError(f"æœªçŸ¥æ¨¡å‹ï¼š{model_name}")
    return model



# å›æ¸¬ç­–ç•¥ï¼šå›å‚³ä¸åŒ TopN çµ„åˆçš„ç­–ç•¥å ±é…¬åºåˆ—
def backtest_strategy(df, selected_features,model):

    strategy_returns = {n: {'long': [], 'short': [], 'long_short': []} for n in [10, 20, 30, 200]}
    for i in range(len(years) - 1):
        train_years = years[:i + 1]
        test_year = years[i + 1]

        train_df = df[df['year'].isin(train_years)]
        test_df = df[df['year'] == test_year]

        X_train = train_df[selected_features]
        y_train = train_df['return']
        X_test = test_df[selected_features]
        y_test = test_df['return']

        model_clone = clone(model)  # æ¯æ¬¡éƒ½è¦ cloneï¼Œé¿å…è¢« overwrite
        model_clone.fit(X_train, y_train)

        test_df = test_df.copy()
        test_df['predicted_return'] = model_clone.predict(X_test)
        test_df['true_return'] = y_test

        for n in [10, 20, 30, 200]:
            top_n = test_df.nlargest(n, 'predicted_return')
            bottom_n = test_df.nsmallest(n, 'predicted_return')

            long_return = top_n['true_return'].mean()
            short_return = -bottom_n['true_return'].mean()
            long_short = (long_return + short_return) / 2

            strategy_returns[n]['long'].append(long_return)
            strategy_returns[n]['short'].append(short_return)
            strategy_returns[n]['long_short'].append(long_short)

    return strategy_returns

def plot_strategies(strategies, best_features, best_parameters, model_name='ridge'):
    plt.figure(figsize=(14, 11))
    best_label = ""
    best_cumret = -np.inf

    markers = {"long": 'o', "short": 'v', "long_short": 's'}
    line_styles = {"long": '-', "short": '--', "long_short": ':'}
    color_map = {'1': 'blue', '10': 'orange', '20': 'green', '30': 'red', '200': 'purple'}
    results_df = pd.DataFrame({'Year': years[1:]})  

    for n in [10, 20, 30, 200]:
        for kind in ['long', 'short', 'long_short']:
            returns = pd.Series(strategies[n][kind])
            cumret = (1 + returns).cumprod()
            label = f'{kind.capitalize()} Top {n}'

            plt.plot(years[1:], cumret, label=label,
                     marker=markers[kind],
                     linestyle=line_styles[kind],
                     color=color_map[str(n)])

            if cumret.iloc[-1] > best_cumret:
                best_cumret = cumret.iloc[-1]
                best_label = label

            results_df[label] = cumret.values

    # ä¸»æ¨™é¡Œèˆ‡åœ–ä¾‹
    plt.title(f'{model_name} GA Optimization - Best Strategy: {best_label} ({best_cumret:.2f})')
    plt.xlabel('Year')
    plt.ylabel('Cumulative Return')
    plt.yscale('log')
    plt.grid(True)
    plt.legend()

    # åŠ å…¥ç‰¹å¾µèˆ‡åƒæ•¸èªªæ˜
    feature_text = 'Best Features:\n' + ', '.join(best_features)
    param_text = 'Best Parameters:\n' + ', '.join([f'{k}={v}' for k, v in best_parameters.items()])

    plt.gcf().text(0.01, 0.01, feature_text, fontsize=10, va='bottom', ha='left', wrap=True)
    plt.gcf().text(0.01, 0.95, param_text, fontsize=10, va='top', ha='left', wrap=True)
    plt.tight_layout(rect=[0.01, 0.05, 1, 0.93])

    # å„²å­˜åœ–èˆ‡æ•¸æ“š
    plt.savefig(os.path.join(result_dir, f'{model_name}_best_strategy_cumulative_returns.png'))
    plt.close()

    csv_path = os.path.join(result_dir, f'{model_name}_cumulative_returns.csv')
    results_df.to_csv(csv_path, index=False)



def backtest_cross_validation(df, selected_features, best_prameters, model_name='ridge'):
    results = []    

    model = init_models_by_name(model_name, best_prameters)

    for i in range(len(years) - 1):
        train_years = years[:i + 1]
        test_years = years[i + 1:]

        train_df = df[df['year'].isin(train_years)]
        strategy_returns = {n: {'long': [], 'short': [], 'long_short': []} for n in [10, 20, 30, 200]}

        for test_year in test_years:
            test_df = df[df['year'] == test_year]
            if test_df.empty:
                continue

            X_train = train_df[selected_features]
            y_train = train_df['return']
            X_test = test_df[selected_features]
            y_test = test_df['return']

            model_clone = clone(model)
            model_clone.fit(X_train, y_train)

            test_df = test_df.copy()
            test_df['predicted_return'] = model_clone.predict(X_test)
            test_df['true_return'] = y_test

            for n in [10, 20, 30, 200]:
                top_n = test_df.nlargest(n, 'predicted_return')
                bottom_n = test_df.nsmallest(n, 'predicted_return')

   
                
                    
                    

                long_return = top_n['true_return'].mean()
                short_return = -bottom_n['true_return'].mean()
                long_short = (long_return + short_return) / 2

                strategy_returns[n]['long'].append(long_return)
                strategy_returns[n]['short'].append(short_return)
                strategy_returns[n]['long_short'].append(long_short)

        # çµæœè¨˜éŒ„
        result_row = {
            'TrainYears': f"{train_years[0]}-{train_years[-1]}",
            'TestYears': f"{test_years[0]}-{test_years[-1]}"
        }

        if len(test_years) == 0:
            continue

        test_year_range = test_years[-1] - test_years[0] + 1

        for n in [10, 20, 30, 200]:
            for strategy_name in ['long', 'short', 'long_short']:
                series = pd.Series(strategy_returns[n][strategy_name])
                if series.empty:
                    result_row[f'Top{n}_{strategy_name}_Cumulative'] = np.nan
                    result_row[f'Top{n}_{strategy_name}_Annual'] = np.nan
                else:
                    cum = (1 + series).prod() - 1
                    ann = float('-1')  # é è¨­å¹´åŒ–å ±é…¬ç‡ç‚º NaN
                    if 1 + cum > 0:
                        ann = (1 + cum) ** (1 / test_year_range) - 1
                    else:
                        ann = float('-1')  # æˆ–è¨­ç‚º 0 æˆ–å…¶ä»–å€¼

                    result_row[f'Top{n}_{strategy_name}_Cumulative'] = round(cum, 4)
                    result_row[f'Top{n}_{strategy_name}_Annual'] = round(ann, 4)

        results.append(result_row)

    return pd.DataFrame(results)





def plot_crossval_results(result_df, base_dir='.', model_name='Model'):

   
    # å„²å­˜ CSV
    csv_path = os.path.join(base_dir, f"{model_name}_crossval_results.csv")
    result_df.to_csv(csv_path, index=False)

    # ç•«å¹´åŒ–å ±é…¬ç‡åœ–
    markers = {"long": 'o', "short": 'v', "long_short": 's'}
    line_styles = {"long": '-', "short": '--', "long_short": ':'}
    color_map = {'1': 'blue', '10': 'orange', '20': 'green', '30': 'red', '200': 'purple'}
    plt.figure(figsize=(14, 10))
    for n in [10, 20, 30, 200]:
        for strategy in ['long', 'short', 'long_short']:
            col_name = f'Top{n}_{strategy}_Annual'
            if col_name in result_df.columns:
                plt.plot(result_df['TestYears'], result_df[col_name], 
                         label=f'Top {n} {strategy.capitalize()}', 
                         marker=markers[strategy], linestyle=line_styles[strategy],
                         color=color_map[str(n)])

    plt.title(f'{model_name} Cross-Validation: Annualized Return')
    plt.xlabel('Test Period')
    plt.ylabel('Annualized Return')
    # plt.yscale('log')  # ä½¿ç”¨å°æ•¸åˆ»åº¦ä»¥ä¾¿æ›´å¥½åœ°é¡¯ç¤ºç´¯ç©å ±é…¬ç‡
    plt.yscale('linear')  # ä½¿ç”¨ç·šæ€§åˆ»åº¦
    plt.grid(True)
    plt.legend()
    plt.xticks(rotation=45)
    plt.tight_layout()
    png_path = os.path.join(base_dir, f"{model_name}_crossval_annual_return.png")
    plt.savefig(png_path)
    plt.close()  # Close the figure to free memory








# --- GA è¨­å®š ---
from joblib import Parallel, delayed
from tqdm import tqdm

population_size =1024
num_generations = 1000

num_features = len(all_features)

# population = np.random.randint(0, 2, size=(population_size, num_features))

# å¹³è¡ŒåŒ–å€‹é«”è©•ä¼°
def evaluate_individual(individual, model_name):
    num_params = len(param_spaces[model_name])
    selected = [f for i, f in enumerate(all_features) if individual[i] == 1]
    if not selected:
        return 0, None, None  # æ²’é¸ç‰¹å¾µç›´æ¥æ·˜æ±°

    try:
        # è§£ç¢¼è¶…åƒæ•¸ï¼ˆå¾ŒåŠæ®µï¼‰
        params = decode_params(param_spaces[model_name], individual[-num_params:])

        # å‹•æ…‹å»ºæ¨¡
        model = init_models_by_name(model_name, params)

        # åŸ·è¡Œç­–ç•¥å›æ¸¬
        result = backtest_strategy(df, selected, model)
        best_cumret = max(
            (1 + pd.Series(result[n][k])).cumprod().iloc[-1]
            for n in [10, 20, 30]
            for k in ['long', 'short', 'long_short']
        )

        return best_cumret, result, selected, params

    except Exception as e:
        print(f"âŒ Error in {model_name}: {str(e)}")
        return 0, None, None, None, None  # è©•ä¼°å¤±æ•—å‰‡è¿”å› 0 åˆ†æ•¸


# ç”¨å¤šæ ¸å¿ƒå¹³è¡Œè·‘ä¸€æ•´ç¾¤å€‹é«”
def safe_evaluate(ind, model_name):
    try:
        return evaluate_individual(ind, model_name)
    except Exception as e:
        print(f"âŒ å€‹é«”è©•ä¼°å¤±æ•—ï¼šmodel={model_name}, ind={ind}, error={e}")
        # å›å‚³é è¨­å€¼ï¼šfitness -infã€ç©ºç­–ç•¥ã€ç©ºç‰¹å¾µã€ç©ºåƒæ•¸ï¼ˆä½ ä¹Ÿå¯ä»¥è‡ªè¨‚ï¼‰
        return -np.inf, None, None, None

def evaluate_population(population, model_name):
    results = Parallel(n_jobs=-1)(  # å»ºè­°å…ˆé™åˆ¶å¹³è¡Œæ•¸é‡ï¼Œä¾‹å¦‚4
        delayed(safe_evaluate)(ind, model_name)
        for ind in tqdm(population, desc=f"Evaluating {model_name}")
    )

    # éæ¿¾æ‰éŒ¯èª¤çš„çµæœï¼ˆä½ ä¹Ÿå¯ä»¥é¸æ“‡ä¿ç•™å†å¾ŒçºŒè™•ç†ï¼‰
    valid_results = [r for r in results if r[1] is not None]

    if not valid_results:
        raise RuntimeError("ğŸ˜­ æ‰€æœ‰å€‹é«”è©•ä¼°éƒ½å¤±æ•—å–µ... æ²’æœ‰æœ‰æ•ˆçµæœ")

    fitness, strategy_history, selected_features_list, param_list = zip(*valid_results)
    return np.array(fitness), list(strategy_history), list(selected_features_list), list(param_list)


def decode_params(param_space, gene_vector):
    param_vec = gene_vector[-len(param_space):]
    decoded = {}
    for i, (key, space) in enumerate(param_space.items()):
        val = param_vec[i]
        if isinstance(space, tuple):
            low, high = space
            decoded[key] = int(low + val * (high - low)) if isinstance(low, int) else low + val * (high - low)
        elif isinstance(space, list):
            idx = int(val * len(space)) % len(space)
            decoded[key] = space[idx]
    return decoded


import pickle

def save_checkpoint(filename, data):
    with open(filename, 'wb') as f:
        pickle.dump(data, f)

def load_checkpoint(filename):
    with open(filename, 'rb') as f:
        return pickle.load(f)

for model_name, model in models.items():
    print(f"Training model: {model_name}")
    checkpoint_path = os.path.join(result_dir, f"{model_name}_checkpoint.pkl")
    cv_result = pd.DataFrame()

    num_params = len(param_spaces[model_name])
    start_generation = 0
    init_density = 0.025

    if os.path.exists(checkpoint_path):
        
        checkpoint = load_checkpoint(checkpoint_path)
        population = checkpoint['population']
        best_score = checkpoint['best_score']
        best_features = checkpoint['best_features']
        best_params = checkpoint['best_params']
        best_strategies = checkpoint['best_strategies']
        no_improvement_count = checkpoint['no_improvement_count']
        start_generation = checkpoint['generation']
        print(f"ğŸ”„ å¾ checkpoint æ¢å¾©: {checkpoint_path} generation: {checkpoint['generation']}")
    else:
        print(f"ğŸ†• é–‹å§‹æ–°çš„è¨“ç·´: {model_name}")
        population = np.random.rand(population_size, num_features + num_params)
        population[:, :num_features] = (population[:, :num_features] < init_density).astype(int)
        best_score = -np.inf
        best_features = None
        best_strategies = None
        best_params = None
        no_improvement_count = 0

    threshold = 300
    delta = 0.001
    init_mutation_rate = 0.2

    for gen in range(start_generation, num_generations):
        if no_improvement_count >= threshold:
            print(f"Stopping early at generation {gen+1} due to no improvement.")
            break
        fitness, all_strategies, selected_features_list, param_list = evaluate_population(population, model_name)

        if np.max(fitness) - best_score <= delta:
            no_improvement_count += 1
        else:
            no_improvement_count = 0

        best_idx = np.argmax(fitness)
        if fitness[best_idx] > best_score:
            old_best_features = best_features if best_features is not None else []
            new_best_features = selected_features_list[best_idx]

            # è¨ˆç®—ç‰¹å¾µè®ŠåŒ–
            added_features = set(new_best_features) - set(old_best_features)
            removed_features = set(old_best_features) - set(new_best_features)
            
            # æ‰“å°ç‰¹å¾µè®ŠåŒ–
            if added_features:
                print(f"+ Added {len(added_features)} features: {', '.join(sorted(added_features))}")
            if removed_features:
                print(f"- Removed {len(removed_features)} features: {', '.join(sorted(removed_features))}")
            best_score = fitness[best_idx]
            best_features = new_best_features
            best_params = param_list[best_idx]
            best_strategies = all_strategies[best_idx]

        # é¸æ“‡
        prob = fitness / fitness.sum()
        indices = np.random.choice(population_size, size=population_size, replace=True, p=prob)
        selected = population[indices]

        # äº¤é…
        next_gen = []
        for i in range(0, population_size, 2):
            p1 = selected[i]
            p2 = selected[(i + 1) % population_size]
            cp = np.random.randint(1, num_features)
            c1 = np.concatenate([p1[:cp], p2[cp:]])
            c2 = np.concatenate([p2[:cp], p1[cp:]])
            next_gen.extend([c1, c2])

        # çªè®Š
        if no_improvement_count >= 100:
            mutation_rate = 1 + no_improvement_count
        elif no_improvement_count >=20:
            mutation_rate = 1 + no_improvement_count / 20
        elif no_improvement_count >= 5:
            mutation_rate = init_mutation_rate + ((0.8 - init_mutation_rate) * (1 - no_improvement_count / 20))
        else:
            mutation_rate = init_mutation_rate

        if mutation_rate/num_features >=1:
            break

        for i in range(population_size):
            for j in range(num_features):
                if np.random.rand() < (mutation_rate / num_features):
                    next_gen[i][j] = 1 - next_gen[i][j]

        population = np.array(next_gen)

        print(f"{model_name} Generation {gen+1}: Best cumulative return = {best_score:.4f}")
        print(f"number of All features: {num_features} no_improvement_count: {no_improvement_count} mutation_rate: {mutation_rate:.4f}")
        print(f"Best features: {best_features} with {len(best_features)} features")
        print(f"Best parameters: {best_params}")


        # ğŸ‘‰ æ¯ä¸€ä»£éƒ½å­˜ checkpoint
        checkpoint_data = {
            'population': population,
            'best_score': best_score,
            'best_features': best_features,
            'best_params': best_params,
            'best_strategies': best_strategies,
            'no_improvement_count': no_improvement_count,
            'generation': gen + 1
        }
        save_checkpoint(checkpoint_path, checkpoint_data)

        # å¯ä»¥é¸æ“‡ä¸è¦æ¯ä¸€ä»£éƒ½åšäº¤å‰é©—è­‰ï¼ˆæœƒå¾ˆæ…¢ï¼‰ï¼Œå¿…è¦æ™‚å†é–‹å•Ÿ
        cv_result = backtest_cross_validation(df, best_features, best_params, model_name)
        plot_strategies(best_strategies, best_features, best_params, model_name)
        plot_crossval_results(cv_result, result_dir, model_name)
        #ç”¨æ¨¡å‹è·‘2024çš„è³‡æ–™
        test_df = df_2024
        if not test_df.empty:
            X_test = test_df[best_features]
            y_test = test_df['return']
            model_clone = clone(model)
            model_clone.set_params(**best_params)
            model_clone.fit(df[df['year'] < 2024][best_features], df[df['year'] < 2024]['return'])
            test_df['predicted_return'] = model_clone.predict(X_test)
            test_df['true_return'] = y_test
            top_10 = test_df.nlargest(10, 'predicted_return')
            top_20 = test_df.nlargest(20, 'predicted_return')
            top_30 = test_df.nlargest(30, 'predicted_return')
            bottom_10 = test_df.nsmallest(10, 'predicted_return')
            bottom_20 = test_df.nsmallest(20, 'predicted_return')
            bottom_30 = test_df.nsmallest(30, 'predicted_return')

            print(f"2025å¹´YTD Top 10 é æ¸¬å ±é…¬ç‡ï¼š{top_10['predicted_return'].mean():.4f}, Top 20ï¼š{top_20['predicted_return'].mean():.4f}, Top 30ï¼š{top_30['predicted_return'].mean():.4f}")

            print(f"2025å¹´YTD Top 10 å¯¦éš›å ±é…¬ç‡ï¼š{top_10['true_return'].mean():.4f}, Top 20ï¼š{top_20['true_return'].mean():.4f}, Top 30ï¼š{top_30['true_return'].mean():.4f}")

            col_names = ['stock_id', 'year_month', 'predicted_return', 'true_return']
            # ä¿ç•™é æ¸¬å’Œå¯¦éš›å ±é…¬ç‡çš„å°æ•¸é»å¾Œ4ä½
            top_Actually = test_df.sort_values(by='true_return', ascending=False).copy()
            top_Actually = top_Actually[col_names].round(4)
            top_predicted = test_df.sort_values(by='predicted_return', ascending=False).copy()
            top_predicted = top_predicted[col_names].round(4)
            top_Actually.to_csv(os.path.join(result_dir, f"{model_name}_2024_top_actual.csv"), index=False)
            top_predicted.to_csv(os.path.join(result_dir, f"{model_name}_2024_top_predicted.csv"), index=False)
            print(f"âœ… 2024å¹´ {model_name} æ¨¡å‹é æ¸¬çµæœå·²å„²å­˜")
        else:
            print(f"âš ï¸ 2024å¹´æ²’æœ‰è³‡æ–™å¯ä¾›æ¸¬è©¦ {model_name} æ¨¡å‹")
                
                
               

    print(f"\nâœ… {model_name} æœ€ä½³ç´¯ç©å ±é…¬ç‡ï¼š", round(best_score, 4))
    print(f"âœ… {model_name} æœ€ä½³ç‰¹å¾µçµ„åˆï¼š", best_features)

    # æœ€çµ‚åšä¸€æ¬¡å®Œæ•´äº¤å‰é©—è­‰èˆ‡åœ–è¡¨å„²å­˜
    cv_result = backtest_cross_validation(df, best_features, best_params, model_name)
    plot_strategies(best_strategies, best_features, best_params, model_name)
    plot_crossval_results(cv_result, result_dir, model_name)



    # åˆªé™¤ checkpointï¼ˆæˆ–ä½ ä¹Ÿå¯ä»¥ä¿ç•™ï¼‰
    # os.remove(checkpoint_path)

import pandas as pd
import yfinance as yf
import os
# import json  # Removed as it is unused
from tqdm import tqdm
from datetime import datetime  # Ensure datetime is used correctly (used in get_last_day and other parts)
import math  # Import math module for mathematical operations
# from threading import Lock  # Removed as it is unused
from concurrent.futures import ThreadPoolExecutor, as_completed
import psycopg2
from dotenv import load_dotenv  # Ensure the 'python-dotenv' package is installed and .env file is properly configured

load_dotenv()
# === DB é€£ç·šè¨­å®š ===
DB_CONFIG = {
    "dbname": os.getenv("DBNAME"),
    "user": os.getenv("USER"),
    "password": os.getenv("PASSWORD"),
    "host": os.getenv("HOST"),
    "port": os.getenv("PORT")
}
print(f"ğŸ”— é€£ç·šåˆ°è³‡æ–™åº« {DB_CONFIG['dbname']} åœ¨ {DB_CONFIG['host']}:{DB_CONFIG['port']}")

try:
    conn = psycopg2.connect(**DB_CONFIG)
    print("âœ… è³‡æ–™åº«é€£ç·šæˆåŠŸ")
except Exception as e:
    print(f"âŒ è³‡æ–™åº«é€£ç·šå¤±æ•—ï¼š{e}")

# # === è·¯å¾‘è¨­å®š ===
# base_dir = 'AIFinTech/Final/gooddata'  # Ensure this path exists if uncommented and replace 'gooddata' with an appropriate directory name if needed
# csv_path = os.path.join(base_dir, 'merged_all_metrics.csv')
# cache_path = os.path.join(base_dir, 'yahoo_price_cache.json')
# output_path = os.path.join(base_dir, 'financial_features_all.csv')
# price_dir = os.path.join(base_dir, 'price_csv')
# os.makedirs(price_dir, exist_ok=True)

# === è®€å–è³‡æ–™ ===
cur = conn.cursor()
cur.execute("ALTER TABLE daily_prices DISABLE TRIGGER trg_refresh_fin_features;")
cur.execute("""
    SELECT s.stock_id,
           MIN(dp.dt) AS first_date,
           MAX(dp.dt) AS last_date
    FROM stocks s
    LEFT JOIN daily_prices dp ON s.stock_id = dp.stock_id
    GROUP BY s.stock_id
""")
rows = cur.fetchall()
df = pd.DataFrame(rows, columns=['ä»£è™Ÿ', 'ç¬¬ä¸€äº¤æ˜“æ—¥', 'æœ€å¾Œäº¤æ˜“æ—¥'])
df['ä»£è™Ÿ'] = df['ä»£è™Ÿ'].astype(int)

# è½‰æˆ Python dateï¼Œè‹¥ç‚º NULL å‰‡å¡«å…¥ 2000-01-01ï¼ˆæ–¹ä¾¿å¾ŒçºŒæ¯”è¼ƒèˆ‡ year å±¬æ€§ï¼‰
default_First_Date = pd.to_datetime('2000-01-01').date()
default_Last_Date = pd.to_datetime('2025-12-31').date()
df['ç¬¬ä¸€äº¤æ˜“æ—¥'] = pd.to_datetime(df['ç¬¬ä¸€äº¤æ˜“æ—¥'], errors='coerce').dt.date.fillna(default_Last_Date)
df['æœ€å¾Œäº¤æ˜“æ—¥'] = pd.to_datetime(df['æœ€å¾Œäº¤æ˜“æ—¥'], errors='coerce').dt.date.fillna(default_First_Date)

# === è‚¡ç¥¨æ¸…å–® ===
all_stock_ids = df['ä»£è™Ÿ'].unique()

def get_last_day(stock_id):
    try:
        cur.execute("SELECT MAX(dt) FROM daily_prices WHERE stock_id = %s", (int(stock_id),))
    except psycopg2.Error as e:
        conn.rollback()
        print(f"âš ï¸ Database error while fetching last day for stock {stock_id}: {e}")
        return None
    result = cur.fetchone()
    return result[0] if result else None


print("ğŸ“¦ æŠ“å–æ¯æª”è‚¡ç¥¨ 2000ï½2024åŠ ä¸Š2025YTD æ‰€æœ‰æ”¶ç›¤åƒ¹...")
# è½‰æˆåŸç”Ÿå‹åˆ¥ tupleï¼š[(stock_id, dt, open, high, low, close, adj, vol), ...]
cached_rows = []

def fetch_and_process_stock(stock_id):

    yf_id = f"{stock_id}.TW"
    hist = None
    # success variable is unused, removing it

    # if os.path.exists(price_csv_path):
    #     try:
    #         hist = pd.read_csv(price_csv_path, parse_dates=['Date'], index_col='Date')
    #     except Exception:
    #         os.remove(price_csv_path)

    # hist_cache variable is unused, removing it

    if hist is None:

        first_year = 2000
        last_year = 2025
        last_date = df.loc[df['ä»£è™Ÿ'] == stock_id, 'æœ€å¾Œäº¤æ˜“æ—¥'].values[0]
        first_year = max(first_year, last_date.year if last_date else first_year)
        today = datetime.now()
        # print(f"last_date: {last_date}, today: {today.date()}")
        if last_date == today.date():
            print(f"âœ… {stock_id} ä»Šå¤©å·²ç¶“æœ‰æœ€æ–°è³‡æ–™ï¼Œè·³éæŠ“å–")
            return f"âœ… {stock_id} è³‡æ–™è™•ç†å®Œæˆ"
        for end_year in range(last_year, first_year - 1, -1):
            for start_year in range(first_year, end_year + 1):
                start_date = (last_date + pd.Timedelta(days=1)).strftime('%Y-%m-%d') if last_date else f"{start_year}-01-01"
                end_date = f"{end_year}-12-31"

                try:
                    hist = yf.download(
                        yf_id,
                        start=start_date,
                        end=end_date,
                        auto_adjust=False,
                        progress=False,
                        timeout=20
                    )
                    if hist is not None and not hist.empty:
                        hist.reset_index(inplace=True)  # Ensure 'Date' is a regular column
                        hist = hist.loc[:, ~hist.columns.duplicated()]

                        print(f"âœ… {stock_id} å¾ {start_date} åˆ° {end_date} æŠ“å–æˆåŠŸ")
                        break
                except Exception as e:
                    print(f"âš ï¸ {stock_id} å¾ {start_date} åˆ° {end_date} æŠ“å–å¤±æ•—ï¼š{e}")
                    continue
            def _to_date_str(val):
                # row['Date'] è‹¥å› é‡è¤‡æ¬„åç­‰ç‹€æ³è®Šæˆ Seriesï¼Œå…ˆå–ç¬¬ä¸€å€‹
                if isinstance(val, pd.Series):
                    val = val.iloc[0]
                ts = pd.to_datetime(val, errors="coerce")
                if pd.isna(ts):
                    return None
                return ts.strftime("%Y-%m-%d")
            def _first_scalar(x):
                # å¦‚æœæ˜¯ Seriesï¼ˆä»£è¡¨æœ‰é‡è¤‡æ¬„åï¼‰ï¼Œæ‹¿ç¬¬ä¸€å€‹ï¼›å¦å‰‡åŸå€¼
                val = x.iloc[0] if isinstance(x, pd.Series) else x
                # å¦‚æœå€¼æ˜¯ NaT æˆ–ç„¡æ•ˆï¼Œè¿”å› None
                if pd.isna(val) or isinstance(val, pd._libs.tslibs.nattype.NaTType):
                    return -1
                return val


            if hist is not None and not hist.empty:
                for _, row in hist.iterrows():
                    stock_id = int(stock_id)
                    dt = _to_date_str(row['Date'])
                    if dt is None:
                        continue
                    op = float(_first_scalar(row['Open']))
                    hi = float(_first_scalar(row['High']))
                    lo = float(_first_scalar(row['Low']))
                    cl = float(_first_scalar(row['Close']))
                    ac = float(_first_scalar(row['Adj Close']))
                    vol_raw = _first_scalar(row['Volume'])
                    vol = int(vol_raw) if (vol_raw is not None and not pd.isna(vol_raw) and not math.isnan(float(vol_raw))) else 0
                    if op==-1 or hi==-1 or lo==-1 or cl==-1 or ac==-1:
                        continue
                    cached_rows.append((stock_id, dt, op, hi, lo, cl, ac, vol))
            else:
                print(f"âš ï¸ {stock_id}: ç„¡æ³•æŠ“å–ä»»ä½•è³‡æ–™ï¼Œè·³éæ­¤è‚¡ç¥¨")

            if hist is not None and not hist.empty:
                hist_end_date = pd.to_datetime(hist['Date'].iloc[-1]).strftime('%Y-%m-%d')
                print(f"âœ… {stock_id} å¾ {start_date} åˆ° {hist_end_date} æŠ“å–æˆåŠŸ")
            else:
                print(f"âš ï¸ {stock_id}: ç„¡æ³•æŠ“å–ä»»ä½•è³‡æ–™ï¼Œè·³éæ­¤è‚¡ç¥¨")
            break

    if hist is None or hist.empty:
        return f"âŒ {stock_id} å®Œå…¨æŠ“ä¸åˆ°ä»»ä½•è³‡æ–™"


    return f"âœ… {stock_id} è³‡æ–™è™•ç†å®Œæˆ"


def fetch_and_process_stock_early(stock_id):
    yf_id = f"{stock_id}.TW"
    hist = None

    # å–å‡ºè³‡æ–™åº«ç´€éŒ„çš„æœ€æ—©äº¤æ˜“æ—¥ï¼ˆç¬¬ä¸€äº¤æ˜“æ—¥ï¼‰
    first_date = df.loc[df['ä»£è™Ÿ'] == stock_id, 'ç¬¬ä¸€äº¤æ˜“æ—¥'].values[0]
    if first_date is None:
        return f"âŒ {stock_id} ç„¡æ³•å–å¾—è³‡æ–™åº«æœ€æ—©äº¤æ˜“æ—¥"

    # å¦‚æœå·²ç¶“æœ‰ 2000 å¹´æˆ–æ›´æ—©çš„è³‡æ–™ï¼Œå‰‡ä¸éœ€è¦å†æŠ“æ›´æ—©çš„åƒ¹æ ¼
    if first_date <= pd.to_datetime('2000-01-04').date():
        print(f"ğŸ” {stock_id} å·²æœ‰ 2000 å¹´æˆ–æ›´æ—©çš„è³‡æ–™ï¼Œè·³éæŠ“å–")
        return f"âœ… {stock_id} è³‡æ–™è™•ç†å®Œæˆï¼ˆç„¡éœ€æŠ“æ›´æ—©ï¼‰"

    # æˆ‘å€‘è¦æŠ“åˆ° first_date ä¹‹å‰ä¸€å¤©ç‚ºæ­¢çš„æ­·å²è³‡æ–™ï¼ˆå¾ 2000-01-01 é–‹å§‹ï¼‰
    fetch_start = "2000-01-01"
    fetch_end_date = (pd.to_datetime(first_date) - pd.Timedelta(days=1)).strftime("%Y-%m-%d")
    start_year = 2000
    end_year = pd.to_datetime(fetch_end_date).year

    # å˜—è©¦ä¸€æ¬¡æŠ“å®Œæ•´å€é–“ï¼›è‹¥å¤±æ•—å†ç”¨é€å¹´ç¸®çŸ­çš„æ–¹å¼é‡è©¦
    for try_end_year in range(end_year, start_year - 1, -1):
        if try_end_year == end_year:
            start = fetch_start
            end = fetch_end_date
        else:
            start = fetch_start
            end = f"{try_end_year}-12-31"

        try:
            hist = yf.download(
                yf_id,
                start=start,
                end=end,
                auto_adjust=False,
                progress=False,
                timeout=20
            )
            if hist is not None and not hist.empty:
                hist.reset_index(inplace=True)  # Ensure 'Date' is a regular column
                hist = hist.loc[:, ~hist.columns.duplicated()]
                print(f"âœ… {stock_id} å¾ {start} åˆ° {end} æŠ“å–æˆåŠŸï¼ˆç”¨æ–¼è£œæŠ“èˆŠè³‡æ–™ï¼‰")
                break
        except Exception as e:
            print(f"âš ï¸ {stock_id} å¾ {start} åˆ° {end} æŠ“å–å¤±æ•—ï¼š{e}")
            continue

    def _to_date_str(val):
        if isinstance(val, pd.Series):
            val = val.iloc[0]
        ts = pd.to_datetime(val, errors="coerce")
        if pd.isna(ts):
            return None
        return ts.strftime("%Y-%m-%d")

    def _first_scalar(x):
        val = x.iloc[0] if isinstance(x, pd.Series) else x
        if pd.isna(val) or isinstance(val, pd._libs.tslibs.nattype.NaTType):
            return -1
        return val

    if hist is None or hist.empty:
        print(f"âš ï¸ {stock_id}: å®Œå…¨æŠ“ä¸åˆ°ä»»ä½•èˆŠè³‡æ–™ï¼ˆå¾ {fetch_start} åˆ° {fetch_end_date}ï¼‰")
        return f"âŒ {stock_id} å®Œå…¨æŠ“ä¸åˆ°ä»»ä½•è³‡æ–™"

    # å°‡æŠ“åˆ°çš„èˆŠè³‡æ–™é€åˆ—åŠ å…¥ cached_rowsï¼ˆæ³¨æ„ä¸è¦è¦†å¯«å¤–å±¤çš„ stock_idï¼‰
    for _, row in hist.iterrows():
        sid = int(stock_id)
        dt = _to_date_str(row['Date'])
        if dt is None:
            continue
        op = float(_first_scalar(row['Open']))
        hi = float(_first_scalar(row['High']))
        lo = float(_first_scalar(row['Low']))
        cl = float(_first_scalar(row['Close']))
        ac = float(_first_scalar(row['Adj Close']))
        vol_raw = _first_scalar(row['Volume'])
        vol = int(vol_raw) if (vol_raw is not None and not pd.isna(vol_raw) and not math.isnan(float(vol_raw))) else 0
        if op == -1 or hi == -1 or lo == -1 or cl == -1 or ac == -1:
            continue
        cached_rows.append((sid, dt, op, hi, lo, cl, ac, vol))

    hist_end_date = pd.to_datetime(hist['Date'].iloc[-1]).strftime('%Y-%m-%d')
    print(f"âœ… {stock_id} è£œæŠ“åˆ°æœ€èˆŠè³‡æ–™ï¼Œå€é–“ {fetch_start} ~ {hist_end_date}")

    return f"âœ… {stock_id} è³‡æ–™è™•ç†å®Œæˆï¼ˆå·²è£œæŠ“èˆŠè³‡æ–™ï¼‰"


# ğŸ¯ å¤šåŸ·è¡Œç·’è·‘èµ·ä¾†
with ThreadPoolExecutor(max_workers=16) as executor:
    futures = {executor.submit(fetch_and_process_stock, int(sid)): int(sid) for sid in all_stock_ids}
    for future in tqdm(as_completed(futures), total=len(futures)):
        result = future.result()
        print(result)

with ThreadPoolExecutor(max_workers=16) as executor:
    futures = {executor.submit(fetch_and_process_stock_early, int(sid)): int(sid) for sid in all_stock_ids}
    for future in tqdm(as_completed(futures), total=len(futures)):
        result = future.result()
        print(result)
    
    




# âœ… åˆ°é€™è£¡å°±å¯ä»¥æ¥ä¸‹ä¾†åšæŒ‡æ¨™é‹ç®—å›‰ï½
print("ğŸ‰ æ‰€æœ‰è‚¡ç¥¨æ­·å²æ”¶ç›¤åƒ¹æŠ“å–å®Œæˆï¼Œå¯é€šçŸ¥è³‡æ–™åº«é–‹å§‹ç‰¹å¾µé‹ç®—äº†å–µâ™¡")
# âœ… åˆ°é€™è£¡å°±å¯ä»¥æ¥ä¸‹ä¾†åšæŒ‡æ¨™é‹ç®—å›‰ï½
print("ğŸ‰ æ‰€æœ‰è‚¡ç¥¨æ­·å²æ”¶ç›¤åƒ¹æŠ“å–å®Œæˆï¼Œå¯é€šçŸ¥è³‡æ–™åº«é–‹å§‹ç‰¹å¾µé‹ç®—äº†å–µâ™¡")

#æš«å­˜cache to .csv
# cached_rows_df = pd.DataFrame(cached_rows, columns=['stock_id', 'dt', 'open_p', 'high_p', 'low_p', 'close_p', 'adj_close', 'volume'])
# cached_rows_df.to_csv("cached_stock_data.csv", index=False)

import io

if cached_rows:
    try:
        # 0) å»º staging è¡¨ï¼ˆåªæœƒåœ¨ä¸å­˜åœ¨æ™‚å»ºç«‹ï¼›UNLOGGED è¼ƒå¿«ï¼‰
        cur.execute("""
        CREATE UNLOGGED TABLE IF NOT EXISTS daily_prices_staging
        (
          stock_id  BIGINT,
          dt        DATE,
          open_p    NUMERIC,
          high_p    NUMERIC,
          low_p     NUMERIC,
          close_p   NUMERIC,
          adj_close NUMERIC,
          volume    BIGINT
        );
        """)

        # 1) æº–å‚™ COPY çš„ç·©è¡
        buf = io.StringIO()
        for r in cached_rows:
            # (stock_id, dt, open, high, low, close, adj_close, volume)
            buf.write(f"{r[0]}\t{r[1]}\t{r[2]}\t{r[3]}\t{r[4]}\t{r[5]}\t{r[6]}\t{r[7]}\n")
        buf.seek(0)

        # 2) æ¸…ç©º stagingï¼Œå°‡æœ¬æ¬¡æ‰¹æ¬¡ COPY é€²å»ï¼ˆæ³¨æ„ï¼šä¸æ˜¯æ¸…ç©ºæ­£è¡¨ï¼‰
        cur.execute("TRUNCATE TABLE daily_prices_staging")
        cur.copy_from(buf, 'daily_prices_staging',
                      columns=['stock_id','dt','open_p','high_p','low_p','close_p','adj_close','volume'])

        # 3) å¾ staging åˆä½µï¼ˆUPSERTï¼‰å›æ­£è¡¨
        cur.execute("""
            INSERT INTO daily_prices AS t
              (stock_id, dt, open_p, high_p, low_p, close_p, adj_close, volume)
            SELECT stock_id, dt, open_p, high_p, low_p, close_p, adj_close, volume
            FROM daily_prices_staging
            ON CONFLICT (stock_id, dt) DO UPDATE SET
              open_p    = EXCLUDED.open_p,
              high_p    = EXCLUDED.high_p,
              low_p     = EXCLUDED.low_p,
              close_p   = EXCLUDED.close_p,
              adj_close = EXCLUDED.adj_close,
              volume    = EXCLUDED.volume;
        """)

        # 4) åˆä½µå®Œæˆå°±æŠŠ staging åˆªæ‰ï¼ˆä½ è¦çš„æ˜¯åˆªé™¤ï¼‰
        cur.execute("DROP TABLE IF EXISTS daily_prices_staging")

        conn.commit()
        print(f"âœ… COPYâ†’UPSERT å®Œæˆï¼Œç­†æ•¸ç´„ {len(cached_rows)}")

    except psycopg2.Error as e:
        conn.rollback()
        print(f"âŒ å¯«å…¥è³‡æ–™åº«å¤±æ•—ï¼š{e}")
    finally:
        # ä¸ç®¡æˆåŠŸèˆ‡å¦éƒ½æŠŠè§¸ç™¼å™¨æ‰“é–‹ï¼ˆé¿å…å¿˜äº†ï¼‰
        try:
            print("ğŸ”„ é–‹å•Ÿè§¸ç™¼å™¨å’Œç‰©åŒ–æª¢è¦–...")
            cur.execute("ALTER TABLE daily_prices ENABLE TRIGGER trg_refresh_fin_features;")
            # å¦‚éœ€åˆ·æ–°ç‰©åŒ–æª¢è¦–ï¼Œå†åŸ·è¡Œï¼ˆå¯è¦–æƒ…æ³ç§»åˆ° try è£¡ï¼‰
            print("ğŸ”„ åˆ·æ–°ç‰©åŒ–æª¢è¦– financial_features...")
            cur.execute("REFRESH MATERIALIZED VIEW CONCURRENTLY financial_features;")
            conn.commit()
        except Exception as e2:
            print(f"âš ï¸ è§¸ç™¼å™¨/ç‰©åŒ–æª¢è¦–æ”¶å°¾æ™‚å‡ºéŒ¯ï¼š{e2}")
else:
    print("âš ï¸ æ²’æœ‰è³‡æ–™éœ€è¦å¯«å…¥è³‡æ–™åº«")
